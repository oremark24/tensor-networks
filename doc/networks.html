
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>3. Tensor Networks &#8212; Tensor Networks for Computer Scientists</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet">
  <link href="../_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.be7d3bbb2ef33a8344ce.js">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.d59cb220de22ca1c485ebbdc042f0030.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"tex": {"macros": {"C": "\\mathbb{C}", "F": "\\mathbb{F}", "Q": "\\mathbb{Q}", "R": "\\mathbb{R}", "cU": "\\mathcal{U}", "cV": "\\mathcal{V}", "cW": "\\mathcal{W}", "cH": "\\mathcal{H}", "cB": "\\mathcal{B}", "cL": "\\mathcal{L}", "def": "\\mathrel{\\vcenter{:}}=", "fed": "=\\mathrel{\\vcenter{:}}", "bold": ["\\pmb{#1}", 1], "norm": ["\\|#1\\|", 1], "Norm": ["\\Bigg\\|#1\\Bigg\\|", 1], "abs": ["\\lvert#1\\rvert", 1], "ip": ["\\langle#1\\vert#2\\rangle", 2], "IP": ["\\Bigg\\langle#1\\,\\Bigg\\vert\\,#2\\Bigg\\rangle", 2], "re": ["\\mathrm{Re}(#1)", 1], "im": ["\\mathrm{Im}(#1)", 1], "tr": "\\mathrm{tr}", "span": "\\mathrm{span}", "inv": "\\mathrm{inv}", "sgn": "\\mathrm{sgn}", "id": "\\mathrm{id}", "diag": "\\mathrm{diag}", "Diag": "\\mathrm{Diag}", "swap": "\\mathrm{SWAP}", "xor": "\\mathrm{XOR}", "and": "\\mathrm{AND}", "or": "\\mathrm{OR}", "copy": "\\mathrm{COPY}", "red": "\\text{red}", "green": "\\text{green}", "blue": "\\text{blue}"}}, "options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="4. Combinatorics" href="combinatorics.html" />
    <link rel="prev" title="2. Tensors" href="tensors.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../_static/logo.svg" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Tensor Networks for Computer Scientists</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="welcome.html">
   Welcome
  </a>
 </li>
</ul>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="foundation.html">
   1. Foundation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="tensors.html">
   2. Tensors
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   3. Tensor Networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="combinatorics.html">
   4. Combinatorics
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="quantum.html">
   5. Quantum Computing
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="mps.html">
   6. Matrix Product State
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="ml.html">
   7. Tensor Machine Learning
  </a>
 </li>
</ul>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="resources.html">
   Resources
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="combinatorics-s.html">
     4. Combinatorics [slides]
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="combinatorics-nb.html">
     4. Combinatorics [notebook]
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="bibliography.html">
   Bibliography
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="todo.html">
   ToDo
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/doc/networks.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
                onclick="printPdf(this)" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/oremark24/tensor-networks"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/oremark24/tensor-networks/issues/new?title=Issue%20on%20page%20%2Fdoc/networks.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show noprint">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#tensor-network-diagrams">
   3.1. Tensor Network Diagrams
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#wiring">
   3.2. Wiring
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#permutations">
   3.3. Permutations
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#graphical-reasoning">
   3.4. Graphical Reasoning
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Tensor Networks</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#tensor-network-diagrams">
   3.1. Tensor Network Diagrams
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#wiring">
   3.2. Wiring
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#permutations">
   3.3. Permutations
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#graphical-reasoning">
   3.4. Graphical Reasoning
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            
              <div>
                
  <div class="tex2jax_ignore mathjax_ignore section" id="tensor-networks">
<h1><span class="section-number">3. </span>Tensor Networks<a class="headerlink" href="#tensor-networks" title="Permalink to this headline">¶</a></h1>
<div class="section" id="tensor-network-diagrams">
<h2><span class="section-number">3.1. </span>Tensor Network Diagrams<a class="headerlink" href="#tensor-network-diagrams" title="Permalink to this headline">¶</a></h2>
<p>In the previous chapter we have introduced tensors and operations on
tensors. Tensor products and contractions offer ways to compose
existing tensors to new ones. Consider for example the order-<span class="math notranslate nohighlight">\((2,1)\)</span> tensor</p>
<div class="math notranslate nohighlight" id="equation-eqnnetwork">
<span class="eqno">(3.1)<a class="headerlink" href="#equation-eqnnetwork" title="Permalink to this equation">¶</a></span>\[N=\sum\limits_{i=1}^n\sum\limits_{k=1}^r\sum\limits_{l=1}^s\sum\limits_{j=1}^m
  T^i_jS^{jl}_k\,e_i\otimes f^k\otimes g_l\,.\]</div>
<p>It was composed from an order-<span class="math notranslate nohighlight">\((1,1)\)</span> tensor
<span class="math notranslate nohighlight">\(T\)</span> and an order-<span class="math notranslate nohighlight">\((2,1)\)</span> tensor
<span class="math notranslate nohighlight">\(S\)</span>. Both have been combined by a
tensor product (yielding 5 tensor indices) and then zipped by a contraction
(yielding 3 remaining tensor indices and the summation index
<span class="math notranslate nohighlight">\(j\)</span>).</p>
<p>In literature various opinions (definitions) are circulating on what
<em>tensor networks</em> are. Some call already a decomposition of a
tensor by means of tensor products and contractions a tensor networks.
For example the <em>Matrix Product State</em>, a decomposition of a
tensor into a sum of matrix products is often referred as a simplistic
instance of a tensor network. Others connect the term tensor network with
the graphical language (notation) we will introduce shortly. Indeed,
<em>tensor network diagrams</em> join shapes representing tensors with
wires. Hence, such diagrams display networks of tensors wired together.
Defining the name is not the most important thing, so we will give a
shaky definition that follows the notion of decomposition into tensors,
but use tensor networks from now on always in conjunction with
illustrations exploiting the graphical language.</p>
<div class="proof definition admonition" id="defNetwork">
<p class="admonition-title"><span class="caption-number">Definition 3.1 </span> (Tensor Network)</p>
<div class="definition-content section" id="proof-content">
<p>A <em><strong>tensor network</strong></em> is a tensor, composed from other tensors using tensor
products and contractions.</p>
</div>
</div><p>Having clarified this, let us straight away jump into the graphical notation
for tensor networks. It was introduced by Roger Penrose in
<span id="id1">[<a class="reference internal" href="bibliography.html#id5" title="Roger Penrose. Applications of negative dimensional tensors. Combinatorial Mathematics and its Applications, pages 221–244, 1971. URL: https://www.mscs.dal.ca/~selinger/papers/graphical-bib/public/Penrose-applications-of-negative-dimensional-tensors.pdf.">Pen71</a>]</span>. Jacob Biamonte has refined the way certain shapes underpin
the structure of tensors <span id="id2">[<a class="reference internal" href="bibliography.html#id3" title="Jacob Biamonte. Lectures on quantum tensor networks. 2019. URL: https://arxiv.org/abs/1912.10049, doi:10.48550/ARXIV.1912.10049.">Bia19</a>]</span>, we will follow mostly his suggestions.</p>
<div class="proof remark admonition" id="illuDiagrams">
<p class="admonition-title"><span class="caption-number">Remark 3.1 </span> (Tensor Network Diagrams)</p>
<div class="remark-content section" id="proof-content">
<p>Each tensor is represented by a geometric shape. Indices are represented by
legs connected to the shape. We will distinguish covariant and contravariant
indices by consideration of the leg direction. Covariant legs point to the
left or up, contravariant legs point to the right or down. The tensor</p>
<div class="math notranslate nohighlight">
\[
T=\sum\limits_{i=1}^n\sum\limits_{j=1}^mT^i_j\,e_i\otimes h^j
\]</div>
<p>can be drawn with left-right orientation as follows.</p>
<a class="reference internal image-reference" href="../_images/intro-t.svg"><img alt="../_images/intro-t.svg" class="align-center" height="39em" src="../_images/intro-t.svg" /></a>
<p>Orienting the legs up-down and describe tensor</p>
<div class="math notranslate nohighlight">
\[
S=\sum\limits_{k=1}^r\sum\limits_{p=1}^m\sum\limits_{l=1}^s
  S^{pl}_k\,f^k\otimes h_p\otimes g_l
\]</div>
<p>as seen here.</p>
<a class="reference internal image-reference" href="../_images/intro-s.svg"><img alt="../_images/intro-s.svg" class="align-center" height="77em" src="../_images/intro-s.svg" /></a>
<p>Also having mixed orientations is possible.</p>
<a class="reference internal image-reference" href="../_images/intro-s2.svg"><img alt="../_images/intro-s2.svg" class="align-center" height="56em" src="../_images/intro-s2.svg" /></a>
<p>The tensor product is displayed by drawing the factor tensor next to each other. We draw</p>
<div class="math notranslate nohighlight">
\[
T\otimes S=
  \sum\limits_{i=1}^n
  \sum\limits_{j=1}^m
  \sum\limits_{k=1}^r
  \sum\limits_{p=1}^m
  \sum\limits_{l=1}^s
  T^i_jS^{pl}_k\,
  e_i\otimes h^j\otimes f^k\otimes h_p\otimes g_l
\]</div>
<p>as follows.</p>
<a class="reference internal image-reference" href="../_images/intro-t-prod-s.svg"><img alt="../_images/intro-t-prod-s.svg" class="align-center" height="83em" src="../_images/intro-t-prod-s.svg" /></a>
<p>The contraction of two indices will be represented by connecting
the respective legs. We will call this a wire. The tensor of equation
<a class="reference internal" href="#equation-eqnnetwork">(3.1)</a> is obtained by</p>
<div class="math notranslate nohighlight">
\[N=C_{j,p}(T\otimes S)\]</div>
<p>and can be displayed as shown in this picture.</p>
<a class="reference internal image-reference" href="../_images/intro-t-prod-s-contracted.svg"><img alt="../_images/intro-t-prod-s-contracted.svg" class="align-center" height="83em" src="../_images/intro-t-prod-s-contracted.svg" /></a>
<p>The naming of indices is an implementation detail that might be omitted
by not labeling certain (or all) legs. In general objects might not be
named if not in focus. Furthermore we will be free to
use symbols, containers, etc. if it supports understanding.</p>
<a class="reference internal image-reference" href="../_images/intro-equation.svg"><img alt="../_images/intro-equation.svg" class="align-center" height="98em" src="../_images/intro-equation.svg" /></a>
</div>
</div><p>So far the general alphabet of graphical tensor language is already defined.
We will now go over some specific words.</p>
<div class="proof remark admonition" id="illuVectors">
<p class="admonition-title"><span class="caption-number">Remark 3.2 </span> (Vectors)</p>
<div class="remark-content section" id="proof-content">
<p>Vectors are represented by triangular shapes, e.g.</p>
<div class="math notranslate nohighlight">
\[
v=\sum\limits_{i=1}^nv^ie_i\in V \,.
\]</div>
<a class="reference internal image-reference" href="../_images/vectors-basic.svg"><img alt="../_images/vectors-basic.svg" class="align-center" height="68em" src="../_images/vectors-basic.svg" /></a>
<p>We will apply this shape also to elements from tensor product spaces
that have only covariant indices, e.g.</p>
<div class="math notranslate nohighlight">
\[\begin{split}
&amp;T=\sum\limits_{i=1}^n\sum\limits_{j=1}^m
  T^{ij}\,e_i\otimes f_j
  \in V\otimes W \,, \\
&amp;S=\sum\limits_{i=1}^n\sum\limits_{j=1}^m\sum\limits_{k=1}^r
  S^{ijk}\,e_i\otimes f_j\otimes g_k
  \in V\otimes W\otimes X \,.
\end{split}\]</div>
<a class="reference internal image-reference" href="../_images/vectors-product.svg"><img alt="../_images/vectors-product.svg" class="align-center" height="68em" src="../_images/vectors-product.svg" /></a>
</div>
</div><div class="proof remark admonition" id="illuDuals">
<p class="admonition-title"><span class="caption-number">Remark 3.3 </span> (Dual Vectors)</p>
<div class="remark-content section" id="proof-content">
<p>Contrarily, the shape is flipped when standing for an element of
a dual space or tensor product of dual spaces, e.g.</p>
<div class="math notranslate nohighlight">
\[\begin{split}
&amp;v=\sum\limits_{i=1}^nv_ie^i\in V^\ast \,, \\ 
&amp;T=\sum\limits_{i=1}^n\sum\limits_{j=1}^m
  T_{ij}\,e^i\otimes f^j
  \in V^\ast\otimes W^\ast \,.
\end{split}\]</div>
<a class="reference internal image-reference" href="../_images/vectors-dual.svg"><img alt="../_images/vectors-dual.svg" class="align-center" height="68em" src="../_images/vectors-dual.svg" /></a>
</div>
</div><div class="proof remark admonition" id="illuGeneral">
<p class="admonition-title"><span class="caption-number">Remark 3.4 </span> (General Tensors)</p>
<div class="remark-content section" id="proof-content">
<p>All tensors combining vector parts and dual parts will usually
be displayed as a square - with exception of specific tensors
that shall be visually distinguishable. We have already seen
examples in <a class="reference internal" href="#illuDiagrams">Remark 3.1</a>, let us
display tensor</p>
<div class="math notranslate nohighlight">
\[
N=\sum\limits_{i=1}^n\sum\limits_{k=1}^r\sum\limits_{l=1}^s 
  N^{il}_k\,e_i\otimes f^k\otimes g_l
\]</div>
<p>from <a class="reference internal" href="#equation-eqnnetwork">(3.1)</a> again.</p>
<a class="reference internal image-reference" href="../_images/general-n.svg"><img alt="../_images/general-n.svg" class="align-center" height="45em" src="../_images/general-n.svg" /></a>
<p>A first species that will own a separate shape to reveal its specifics are
tensors with diagonal coefficients matrix, e.g.</p>
<div class="math notranslate nohighlight">
\[
d=\sum\limits_{i=1}^nd^i_i\,e_i\otimes e^i\in V\otimes V^\ast\,.
\]</div>
<p>These diagonal tensors are, so to speak, a hybrid of a vector and
a tensor having a covariant as well as a contravariant index. This
is reflected by combining the shapes of a vector and a dual vector
into a diamond shape.</p>
<a class="reference internal image-reference" href="../_images/general-diagonal.svg"><img alt="../_images/general-diagonal.svg" class="align-center" height="56em" src="../_images/general-diagonal.svg" /></a>
</div>
</div><p>Now we are ready to combine tensors by connecting legs (indices) with
wires (contractions). A fundamental ingredient of Linear Algebra
are linear maps. In the previous chapter we have seen, how they can
be formulated in terms of tensors. We will translate the formulas
obtained there into the graphical tensor language. The outcome
will match our intuition.</p>
<div class="proof remark admonition" id="illuLinMaps">
<p class="admonition-title"><span class="caption-number">Remark 3.5 </span> (Linear Maps)</p>
<div class="remark-content section" id="proof-content">
<p>We recall, that we can write the application of a linear map
<span class="math notranslate nohighlight">\(T\)</span> to a vector <span class="math notranslate nohighlight">\(v\)</span> as the tensor product of both regarded as tensors:</p>
<div class="math notranslate nohighlight">
\[
T(v)=C_{i,k}(T\otimes v)\,.
\]</div>
<p>Hence, this can be drawn as follows.</p>
<a class="reference internal image-reference" href="../_images/lin-maps-Tv.svg"><img alt="../_images/lin-maps-Tv.svg" class="align-center" height="68em" src="../_images/lin-maps-Tv.svg" /></a>
<p>Similarly, we can visualize the application of a dual map</p>
<div class="math notranslate nohighlight">
\[
T^\ast(v^\ast)=C_{k,j}(T\otimes v^\ast)
\]</div>
<p>as:</p>
<a class="reference internal image-reference" href="../_images/lin-maps-vT.svg"><img alt="../_images/lin-maps-vT.svg" class="align-center" height="68em" src="../_images/lin-maps-vT.svg" /></a>
<p>Actually, the same tensor can be interpreted in both ways.
If we connect the covariant index with a vector, then it
is acting as linear map. The remaining open index of the
construct is a contravariant index, symbolizing the
image vector. If we connect the contravariant index of the
tensor with a dual vector, then we have visualized
the application of a dual linear map. The resulting image
possesses a covariant index, thus being the dual image
vector.</p>
<p>Chaining linear maps is straight forward as well. This is
achieved by contracting the tensor product of the tensors
representing the maps.</p>
<div class="math notranslate nohighlight">
\[
(S\circ T)(v)=C_{p,j}(C_{i,k}(S\otimes T\otimes v))
\]</div>
<p>Hence, the picture shows what intution expects, the
two tensors symbolizing the maps have to be wired together.</p>
<a class="reference internal image-reference" href="../_images/lin-maps-STv.svg"><img alt="../_images/lin-maps-STv.svg" class="align-center" height="68em" src="../_images/lin-maps-STv.svg" /></a>
<p>Hence, the picture shows what intution expects, the
two tensors symbolizing the maps have to be wired together.</p>
<p>A bilinear map</p>
<div class="math notranslate nohighlight">
\[
T(v_1,v_2)=C_{i_1,k_1}(C_{i_2,k_2}(T\otimes v_1\otimes v_2))
\]</div>
<p>extends this picture simply by adding and contracting the second argument
to the map tensor.</p>
<a class="reference internal image-reference" href="../_images/lin-maps-Tvw.svg"><img alt="../_images/lin-maps-Tvw.svg" class="align-center" height="90em" src="../_images/lin-maps-Tvw.svg" /></a>
<p>Multilinear maps would extend this picture even further by adding as many
argument vectors as needed.</p>
<p>In case that the map tensor can be decomposed into a tensor product,
enabling equation</p>
<div class="math notranslate nohighlight">
\[
(T_1\otimes T_2)(v_1\otimes v_2)=T_1(v_1)\otimes T_2(v_2)\,,
\]</div>
<p>the decomposition is shown as two maps next to each other. Note, that
this complies with the figure of a tensor product - both
tensors will be drawn independently without any interaction (as long as no
contraction is involved).</p>
<a class="reference internal image-reference" href="../_images/lin-maps-TvSw.svg"><img alt="../_images/lin-maps-TvSw.svg" class="align-center" height="90em" src="../_images/lin-maps-TvSw.svg" /></a>
</div>
</div><div class="proof remark admonition" id="illuScalar">
<p class="admonition-title"><span class="caption-number">Remark 3.6 </span> (Scalars, Complete Contraction)</p>
<div class="remark-content section" id="proof-content">
<p>So far we have not considered the possibility of a scalar (element of the underlying field).
Since a scalar value can be treated as tensor without indices, it is exactly drawn like this,
as shape without legs.</p>
<a class="reference internal image-reference" href="../_images/scalar.svg"><img alt="../_images/scalar.svg" class="align-center" height="48em" src="../_images/scalar.svg" /></a>
<p>Having used a circle is arbitrary - actually it does not
matter, because we will very rarely draw isoloated
scalar tensors. More common is the case, that a complex
tensor network is fully contracted to a scalar. We give a
few examples (with limited complexity though) here.
Referring to <a class="reference internal" href="tensors.html#exMatrixContraction">Example 2.3</a>, the
trace of a matrix can be expressed as fully contracted
tensor:</p>
<div class="math notranslate nohighlight">
\[
\tr(T)=C_{i,j}(T) \,.
\]</div>
<a class="reference internal image-reference" href="../_images/scalar-trace.svg"><img alt="../_images/scalar-trace.svg" class="align-center" height="54em" src="../_images/scalar-trace.svg" /></a>
<p>Using the way of visualizing composed linear maps, the <em>trace</em> of
a matrix product (composed linear maps) would be:</p>
<a class="reference internal image-reference" href="../_images/scalar-trace2.svg"><img alt="../_images/scalar-trace2.svg" class="align-center" height="57em" src="../_images/scalar-trace2.svg" /></a>
<p>Another operation from Linear Algebra that results in a scalar value is the <em>inner product</em>.
We consider the case of the inner product space being <span class="math notranslate nohighlight">\(\C^n\)</span> equipped with the standard basis
(which is orthonormal) <span class="math notranslate nohighlight">\(\{e_1,\ldots,e_n\}\)</span>. We can calculate the inner product of
<span class="math notranslate nohighlight">\(v=\sum\limits_{i=1}^nv^ie_i\)</span> and <span class="math notranslate nohighlight">\(w=\sum\limits_{j=1}^nw^je_j\)</span> by their coefficients:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\ip{v}{w} 
&amp;= \IP{\sum\limits_{i=1}^nv^ie_i}{\sum\limits_{j=1}^nw^je_j} \\
&amp;= \sum\limits_{i=1}^n\sum\limits_{j=1}^n
  \bar{v}^iw^j\underbrace{\ip{e_i}{e_j}}_{=\delta_{ij}} \\
&amp;= \sum\limits_{i=1}^n\bar{v}^iw^i\,.
\end{split}\]</div>
<p>On the other hand, let <span class="math notranslate nohighlight">\(\bar{v}=\sum\limits_{i=1}^n\bar{v}_ie^i\)</span>
be a dual vector, that has the conjugate complex coefficients of <span class="math notranslate nohighlight">\(v\)</span>
(i.e. <span class="math notranslate nohighlight">\(\bar{v}^i=\bar{v}_i\)</span>) in the dual standard basis. We calculate</p>
<div class="math notranslate nohighlight">
\[\begin{split}
C_{i,j}(\bar{v}\otimes w)
&amp;= C_{i,j}\Bigg(\sum\limits_{i=1}^n\sum\limits_{j=1}^n
  \bar{v}_iw^j\,e^i\otimes e_j\Bigg) \\
&amp;= \sum\limits_{i=1}^n\sum\limits_{j=1}^n
  \bar{v}_iw^j\underbrace{e^i(e_j)}_{=\delta^i_j} \\
&amp;= \sum\limits_{i=1}^n\underbrace{\bar{v}_i}_{=\bar{v}^i}w^i \\
&amp;= \ip{v}{w} \,.
\end{split}\]</div>
<p>Thus, the inner product of <span class="math notranslate nohighlight">\(v\)</span> and <span class="math notranslate nohighlight">\(w\)</span> can be expressed with a tensor product
using <span class="math notranslate nohighlight">\(\bar{v}\)</span> instead of <span class="math notranslate nohighlight">\(v\)</span>. The tensor network diagram can be drawn accordingly.</p>
<a class="reference internal image-reference" href="../_images/scalar-ip.svg"><img alt="../_images/scalar-ip.svg" class="align-center" height="68em" src="../_images/scalar-ip.svg" /></a>
</div>
</div><p>With this we have introduced the basic building blocks of tensor network
diagrams. As we see, they provide a bird’s eye perspective on tensor networks.
Certain implementation details are abstracted away from the viewer, such as
index labels or index order. For example, the tensor behind</p>
<a class="reference internal image-reference" href="../_images/general-t.svg"><img alt="../_images/general-t.svg" class="align-center" height="45em" src="../_images/general-t.svg" /></a>
<p>could be</p>
<div class="math notranslate nohighlight">
\[
T=\sum\limits_{i=1}^n\sum\limits_{j=1}^m\sum\limits_{k=1}^r 
  T^{ij}_k\,e_i\otimes f_j\otimes g^k \in V\otimes W\otimes X^\ast
  \,,
\]</div>
<p>or with other index names</p>
<div class="math notranslate nohighlight">
\[
T=\sum\limits_{p=1}^n\sum\limits_{q=1}^m\sum\limits_{t=1}^r 
  T^{pq}_t\,e_p\otimes f_q\otimes g^t \in V\otimes W\otimes X^\ast
  \,,
\]</div>
<p>or changing the index order</p>
<div class="math notranslate nohighlight">
\[
T=\sum\limits_{k=1}^r \sum\limits_{j=1}^m\sum\limits_{i=1}^n
  T^{ji}_k\,g^k\otimes f_j\otimes e_i \in X^\ast\otimes W\otimes V
  \,,
\]</div>
<p>or an object in another space</p>
<div class="math notranslate nohighlight">
\[
T=\sum\limits_{i=1}^n\sum\limits_{j=1}^n\sum\limits_{k=1}^n
  T^{ij}_k\,e_i\otimes e_j\otimes e^k \in \R^n\otimes\R^n\otimes\R^n
  \,.
\]</div>
<p>However, we want to emphasize that certain properties are strict and do not allow for
ambiguity. The following tensor network diagrams refer to different object because the
amounts of covariant and contravariant (the tensor’s order) do matter.</p>
<a class="reference internal image-reference" href="../_images/general-neq.svg"><img alt="../_images/general-neq.svg" class="align-center" height="44em" src="../_images/general-neq.svg" /></a>
<p>But we need to be careful, indices might not be interchangeable when tensor’s internal
mode of operation is known and not symmetric. Consider e.g.
<span class="math notranslate nohighlight">\(T=e^1\otimes e^2\in\R^2\otimes\R^2\)</span> with <span class="math notranslate nohighlight">\(\{e^1,e^2\}\)</span> being dual standard
basis. Then the diagrams</p>
<a class="reference internal image-reference" href="../_images/general-neq2.svg"><img alt="../_images/general-neq2.svg" class="align-center" height="105em" src="../_images/general-neq2.svg" /></a>
<p>are not equal, the left one contracts to value <span class="math notranslate nohighlight">\(1\)</span>, the right one contracts to value <span class="math notranslate nohighlight">\(0\)</span>.</p>
</div>
<div class="section" id="wiring">
<h2><span class="section-number">3.2. </span>Wiring<a class="headerlink" href="#wiring" title="Permalink to this headline">¶</a></h2>
<p>The graphical representation of a tensor network can be considered as a graph, consisting
of nodes and wires, with potentially open wires coming out of a tensor but not leading
to another tensor. After having described different kinds of nodes, we will deal with the
wires in this section.</p>
<p>First, we recall the definition of the <em>Kronecker delta</em>, <a class="reference internal" href="tensors.html#def-kronecker">Definition 2.5</a>. Now,
we will consider the Kronecker delta as tensor by using for example <span class="math notranslate nohighlight">\(\delta_i^j\)</span> as
coefficients. This results in tensor</p>
<div class="math notranslate nohighlight" id="equation-eq-kronecker-delta-tensor">
<span class="eqno">(3.2)<a class="headerlink" href="#equation-eq-kronecker-delta-tensor" title="Permalink to this equation">¶</a></span>\[\sum\limits_{i=1}^n\sum\limits_{j=1}^n\delta^i_j\,e_i\otimes e^j\]</div>
<p>that can be drawn as:</p>
<div class="figure align-center" id="fig-tensor-networks-kronecker-delta">
<a class="reference internal image-reference" href="../_images/kronecker-delta.svg"><img alt="../_images/kronecker-delta.svg" height="39em" src="../_images/kronecker-delta.svg" /></a>
<p class="caption"><span class="caption-number">Fig. 3.1 </span><span class="caption-text">Kronecker delta</span><a class="headerlink" href="#fig-tensor-networks-kronecker-delta" title="Permalink to this image">¶</a></p>
</div>
<p>Usually the Kronecker delta will be used in situations when both indices share the
same dimension. This is the reason, that both summation indices of equation
<a class="reference internal" href="#equation-eq-kronecker-delta-tensor">(3.2)</a> have the same limits.</p>
<p>Now, assume we have other tensors connected to the Kronecker delta, for example:</p>
<div class="figure align-center" id="fig-tensor-networks-kronecker-connected">
<a class="reference internal image-reference" href="../_images/kronecker-connected.svg"><img alt="../_images/kronecker-connected.svg" height="39em" src="../_images/kronecker-connected.svg" /></a>
<p class="caption"><span class="caption-number">Fig. 3.2 </span><span class="caption-text">Connected Kronecker delta</span><a class="headerlink" href="#fig-tensor-networks-kronecker-connected" title="Permalink to this image">¶</a></p>
</div>
<p>The Kronecker delta will only keep coefficients with <span class="math notranslate nohighlight">\(i=j\)</span> alive. This means, that
the two contractions along <span class="math notranslate nohighlight">\(i\)</span> and <span class="math notranslate nohighlight">\(j\)</span> will be resolved into one contraction along
a unified index:</p>
<div class="math notranslate nohighlight">
\[
\sum\limits_{k=1}^r\sum\limits_{l=1}^s\sum\limits_{i=1}^n\sum\limits_{j=1}^n
  T^k_i\delta^i_jS^j_l\,f_k\otimes g^l
= \sum\limits_{k=1}^r\sum\limits_{l=1}^s\sum\limits_{i=1}^n
  T^k_iS^i_l\,f_k\otimes g^l
\,.
\]</div>
<p>This equation can also be expressed within the diagramatic language:</p>
<div class="figure align-center" id="fig-tensor-networks-kronecker-equation">
<a class="reference internal image-reference" href="../_images/kronecker-equation.svg"><img alt="../_images/kronecker-equation.svg" height="39em" src="../_images/kronecker-equation.svg" /></a>
<p class="caption"><span class="caption-number">Fig. 3.3 </span><span class="caption-text">Resolution of Kronecker delta</span><a class="headerlink" href="#fig-tensor-networks-kronecker-equation" title="Permalink to this image">¶</a></p>
</div>
<p>In consequence the Kronecker delta is in a sense representing a wire. The directions are
defined by the type of Kronecker delta (location of the indices).</p>
<div class="proof remark admonition" id="rem-tensor-networks-kronecker">
<p class="admonition-title"><span class="caption-number">Remark 3.7 </span> (Drawing the Kronecker delta)</p>
<div class="remark-content section" id="proof-content">
<p>We have the following diagramatic representations of the Kronecker delta versions.</p>
<div class="figure align-center" id="fig-tensor-networks-kronecker-line">
<a class="reference internal image-reference" href="../_images/kronecker-line.svg"><img alt="../_images/kronecker-line.svg" height="64em" src="../_images/kronecker-line.svg" /></a>
<p class="caption"><span class="caption-number">Fig. 3.4 </span><span class="caption-text">Kronecker delta as map</span><a class="headerlink" href="#fig-tensor-networks-kronecker-line" title="Permalink to this image">¶</a></p>
</div>
<div class="figure align-center" id="fig-tensor-networks-kronecker-left">
<a class="reference internal image-reference" href="../_images/kronecker-left.svg"><img alt="../_images/kronecker-left.svg" height="64em" src="../_images/kronecker-left.svg" /></a>
<p class="caption"><span class="caption-number">Fig. 3.5 </span><span class="caption-text">Kronecker delta as vector</span><a class="headerlink" href="#fig-tensor-networks-kronecker-left" title="Permalink to this image">¶</a></p>
</div>
<div class="figure align-center" id="fig-tensor-networks-kronecker-right">
<a class="reference internal image-reference" href="../_images/kronecker-right.svg"><img alt="../_images/kronecker-right.svg" height="64em" src="../_images/kronecker-right.svg" /></a>
<p class="caption"><span class="caption-number">Fig. 3.6 </span><span class="caption-text">Kronecker delta as dual vector</span><a class="headerlink" href="#fig-tensor-networks-kronecker-right" title="Permalink to this image">¶</a></p>
</div>
</div>
</div><p>We have justified <a class="reference internal" href="#fig-tensor-networks-kronecker-line"><span class="std std-numref">Fig. 3.4</span></a>, but what about
<a class="reference internal" href="#fig-tensor-networks-kronecker-left"><span class="std std-numref">Fig. 3.5</span></a> and <a class="reference internal" href="#fig-tensor-networks-kronecker-right"><span class="std std-numref">Fig. 3.6</span></a>?
After all we have worked out that contractions always combine a contravariant
and a covariant index. So, how can a bended wire represent a contraction? This
question is answered by the Kronecker delta interpretation. Consider for example the
following situation.</p>
<div class="figure align-center" id="fig-tensor-networks-kronecker-bended">
<a class="reference internal image-reference" href="../_images/kronecker-bended.svg"><img alt="../_images/kronecker-bended.svg" height="114em" src="../_images/kronecker-bended.svg" /></a>
<p class="caption"><span class="caption-number">Fig. 3.7 </span><span class="caption-text">Bended contraction</span><a class="headerlink" href="#fig-tensor-networks-kronecker-bended" title="Permalink to this image">¶</a></p>
</div>
<p>This looks like a linear map <span class="math notranslate nohighlight">\(T\)</span> applied to a dual vector <span class="math notranslate nohighlight">\(v^*\)</span>. With help of
<span class="math notranslate nohighlight">\(\sum\limits_{i=1}^n\sum\limits_{j=1}^n\delta^{ij}\,e_i\otimes e_j\)</span> which is
standing for the bending to the left, we can describe this tensor network algebraically:</p>
<div class="math notranslate nohighlight">
\[
\sum\limits_{k=1}^m\sum\limits_{i=1}^n\sum\limits_{j=1}^n
  T^k_i\delta^{ij}v^\ast_j\,f_k
= \sum\limits_{k=1}^m\sum\limits_{i=1}^n
  T^k_iv^\ast_i\,f_k
\,.
\]</div>
<p>The intermediate Kronecker delta is adapting the index locations. Thus, the diagram
<a class="reference internal" href="#fig-tensor-networks-kronecker-bended"><span class="std std-numref">Fig. 3.7</span></a> is completely justified. It appears as
either the index <span class="math notranslate nohighlight">\(i\)</span> of <span class="math notranslate nohighlight">\(T\)</span> was rasied to superscript or as index <span class="math notranslate nohighlight">\(j\)</span> of <span class="math notranslate nohighlight">\(v^\ast\)</span> was
lowered to subscript (so that the application of linear map to dual vector can be
executed). Accordingly this technique is called <em>raising</em> respectively <em>lowering</em> indices.
Raising and lowering indices preserves the coefficients of a tensor, but uses it together
with the dual basis (for the modified index).</p>
<p>If we consider vector <span class="math notranslate nohighlight">\(\sum\limits_{k=1}^nv^ke_k\)</span>, then lowering the index (of coefficients)
would be achieved by following equation:</p>
<div class="math notranslate nohighlight" id="equation-eq-tensor-networks-lowering-index">
<span class="eqno">(3.3)<a class="headerlink" href="#equation-eq-tensor-networks-lowering-index" title="Permalink to this equation">¶</a></span>\[\begin{split}C_{j,k}\Bigg(
  \Bigg[\sum\limits_{i=1}^n\sum\limits_{j=1}^n\delta_{ij}\,e^i\otimes e^j\Bigg]
  \otimes\Bigg[\sum\limits_{k=1}^nv^ke_k\Bigg]
\Bigg) 
&amp;= \sum\limits_{i=1}^n\sum\limits_{j=1}^n\sum\limits_{k=1}^n\delta_{ij}v^k
  \underbrace{e^j(e_k)}_{=\delta^j_k}e^i \\
&amp;= \sum\limits_{i=1}^n\sum\limits_{j=1}^n\delta_{ij}v^je^i \\
&amp;= \sum\limits_{i=1}^nv^ie^i \\
&amp;\fed v' \,.\end{split}\]</div>
<p>We see, that we have constructed a dual vector <span class="math notranslate nohighlight">\(v'\)</span> with same coefficients <span class="math notranslate nohighlight">\(v^i\)</span>. It
would be convenient to write them with a subscript index now. On the other hand,
a dual vector is representing a linear functional with its matrix representation
being a row vector. Keeping the basis fixed, we identify</p>
<div class="math notranslate nohighlight">
\[\begin{split}
v=
\begin{bmatrix}
  v^1 \\
  \vdots \\
  v^n
\end{bmatrix}\,.
\end{split}\]</div>
<p>The constructed dual vector, having the same coefficients, is represented by</p>
<div class="math notranslate nohighlight">
\[
v'=
\begin{bmatrix}
  v^1 &amp; \cdots &amp; v^n
\end{bmatrix}
=v^T\,.
\]</div>
<p>Hence, we could write <span class="math notranslate nohighlight">\(v'=\sum\limits_{i=1}^nv^T_ie^i\)</span>. Bending the wire, which
implies lowering the index, is transposing the coefficients vector. The same
can be done for a dual vector and a linear map (matrix). Using the tensor
network diagram notation, the relations can be expressed as follows.</p>
<div class="proof observation admonition" id="obs-tensor-networks-transposition">
<p class="admonition-title"><span class="caption-number">Observation 3.1 </span> (Transposition)</p>
<div class="observation-content section" id="proof-content">
<div class="figure align-center" id="fig-tensor-networks-transposition-vector">
<a class="reference internal image-reference" href="../_images/transposition-vector.svg"><img alt="../_images/transposition-vector.svg" height="84em" src="../_images/transposition-vector.svg" /></a>
<p class="caption"><span class="caption-number">Fig. 3.8 </span><span class="caption-text">Vector transposition</span><a class="headerlink" href="#fig-tensor-networks-transposition-vector" title="Permalink to this image">¶</a></p>
</div>
<div class="figure align-center" id="fig-tensor-networks-transposition-dual">
<a class="reference internal image-reference" href="../_images/transposition-dual.svg"><img alt="../_images/transposition-dual.svg" height="84em" src="../_images/transposition-dual.svg" /></a>
<p class="caption"><span class="caption-number">Fig. 3.9 </span><span class="caption-text">Dual vector transposition</span><a class="headerlink" href="#fig-tensor-networks-transposition-dual" title="Permalink to this image">¶</a></p>
</div>
<div class="figure align-center" id="fig-tensor-networks-transposition-matrix">
<a class="reference internal image-reference" href="../_images/transposition-matrix.svg"><img alt="../_images/transposition-matrix.svg" height="78em" src="../_images/transposition-matrix.svg" /></a>
<p class="caption"><span class="caption-number">Fig. 3.10 </span><span class="caption-text">Linear map transposition</span><a class="headerlink" href="#fig-tensor-networks-transposition-matrix" title="Permalink to this image">¶</a></p>
</div>
</div>
</div><p>We wanted to be precise in <a class="reference internal" href="#obs-tensor-networks-transposition">Observation 3.1</a>, but considering
<a class="reference internal" href="#fig-tensor-networks-transposition-vector"><span class="std std-numref">Fig. 3.8</span></a> and
<a class="reference internal" href="#fig-tensor-networks-transposition-dual"><span class="std std-numref">Fig. 3.9</span></a>, the vectors of coefficients <span class="math notranslate nohighlight">\(v\)</span> and <span class="math notranslate nohighlight">\(v^T\)</span>
are the same - apart from orientation. In future diagrams we might be lazy and stick in
both cases to <span class="math notranslate nohighlight">\(v\)</span> (if there is no danger of confusion). <span id="id3">[<a class="reference internal" href="bibliography.html#id5" title="Roger Penrose. Applications of negative dimensional tensors. Combinatorial Mathematics and its Applications, pages 221–244, 1971. URL: https://www.mscs.dal.ca/~selinger/papers/graphical-bib/public/Penrose-applications-of-negative-dimensional-tensors.pdf.">Pen71</a>]</span> gives the
following identity, called snake or zig-zag equation.</p>
<div class="proof observation admonition" id="obs-tensor-networks-snake-equation">
<p class="admonition-title"><span class="caption-number">Observation 3.2 </span> (Snake equation)</p>
<div class="observation-content section" id="proof-content">
<div class="figure align-center" id="fig-tensor-networks-transposition-snake">
<a class="reference internal image-reference" href="../_images/transposition-snake.svg"><img alt="../_images/transposition-snake.svg" height="78em" src="../_images/transposition-snake.svg" /></a>
<p class="caption"><span class="caption-number">Fig. 3.11 </span><span class="caption-text">Snake equation</span><a class="headerlink" href="#fig-tensor-networks-transposition-snake" title="Permalink to this image">¶</a></p>
</div>
</div>
</div><div class="proof admonition" id="proof">
<p>Proof. We have <span class="math notranslate nohighlight">\(\delta^{ik}\delta_{kj}\neq 0\)</span> if and only if <span class="math notranslate nohighlight">\(i=k=j\)</span>. Therefore,</p>
<div class="math notranslate nohighlight">
\[
\sum\limits_{i=1}^n\sum\limits_{j=1}^n\sum\limits_{k=1}^n\delta^{ik}\delta_{kj}\,e_i\otimes e^j
=\sum\limits_{i=1}^n\sum\limits_{j=1}^n\delta^i_j\,e_i\otimes e^j \,.
\]</div>
</div>
<p>Next we will use the tensor</p>
<div class="math notranslate nohighlight" id="equation-eqn-tensor-networks-swap-tensor">
<span class="eqno">(3.4)<a class="headerlink" href="#equation-eqn-tensor-networks-swap-tensor" title="Permalink to this equation">¶</a></span>\[S=\sum\limits_{i=1}^n\sum\limits_{j=1}^n\sum\limits_{k=1}^n\sum\limits_{l=1}^n
  \delta^i_l\delta^j_k\,e_i\otimes e_j\otimes e^k\otimes e^l\,.\]</div>
<p>For <span class="math notranslate nohighlight">\(v=\sum\limits_{p=1}^nv^pe_p\in V\)</span> and <span class="math notranslate nohighlight">\(w=\sum\limits_{q=1}^nw^qe_q\in W\)</span> we define the
<span class="math notranslate nohighlight">\(\swap\)</span> operator by</p>
<div class="math notranslate nohighlight" id="equation-eqn-tensor-networks-swap-operator">
<span class="eqno">(3.5)<a class="headerlink" href="#equation-eqn-tensor-networks-swap-operator" title="Permalink to this equation">¶</a></span>\[\swap:v\otimes w\longmapsto C_{k,p}(C_{l,q}(S\otimes v\otimes w))\,.\]</div>
<p>Evaluating the contractions we obtain</p>
<div class="math notranslate nohighlight" id="equation-eqn-tensor-networks-swap-resolution">
<span class="eqno">(3.6)<a class="headerlink" href="#equation-eqn-tensor-networks-swap-resolution" title="Permalink to this equation">¶</a></span>\[\begin{split}\swap(v\otimes w) 
&amp;= \sum\limits_{i=1}^n\sum\limits_{j=1}^n\sum\limits_{k=1}^n\sum\limits_{l=1}^n
  \delta^i_l\delta^j_kv^kw^l\,e_i\otimes e_j \\
&amp;= \sum\limits_{i=1}^n\sum\limits_{j=1}^nv^jw^i\,e_i\otimes e_j \\
&amp;= \Bigg(\sum\limits_{i=1}^nw^ie_i\Bigg)\otimes \Bigg(\sum\limits_{j=1}^nv^je_j\Bigg) \\
&amp;= w\otimes v\,.\end{split}\]</div>
<p>Hence the <span class="math notranslate nohighlight">\(\swap\)</span> operator is a linear map (bilinear if we consider <span class="math notranslate nohighlight">\(v\)</span> and <span class="math notranslate nohighlight">\(w\)</span> to be
independent variables) with</p>
<div class="math notranslate nohighlight" id="equation-eqn-tensor-networks-swap-meta">
<span class="eqno">(3.7)<a class="headerlink" href="#equation-eqn-tensor-networks-swap-meta" title="Permalink to this equation">¶</a></span>\[\begin{split}\swap: &amp;\, V\otimes W\longrightarrow W\otimes V\,,\\
  &amp;\, v\otimes w\longmapsto w\otimes v\,.\end{split}\]</div>
<p>During chapter <a class="reference internal" href="tensors.html#ch-tensors"><span class="std std-ref">Tensors</span></a> we have stated that the tensor product can be
consider as commutative. We will not question this fact, as
<span class="math notranslate nohighlight">\(V\otimes W\simeq W\otimes V\)</span> with the natural isomorphism
<span class="math notranslate nohighlight">\(v\otimes w\leftrightarrow w\otimes v\)</span>. In practical applications, however,
the order of factors might be relevant - because it might define which
tensors in the diagram are connected. In quantum circuits for example
qubits might not be interchangeable. In these cases, the <span class="math notranslate nohighlight">\(\swap\)</span> operator is
relevant - if values of two qubits need to be swapped during a quantum computation
for instance. In chapter <a class="reference internal" href="quantum.html#ch-quantum"><span class="std std-ref">Quantum Computing</span></a> we will learn how the <span class="math notranslate nohighlight">\(\swap\)</span> operator
can implemented on quantum computers. For the moment we leave it with claiming
its relevance.</p>
<p>Equations <a class="reference internal" href="#equation-eqn-tensor-networks-swap-tensor">(3.4)</a>,
<a class="reference internal" href="#equation-eqn-tensor-networks-swap-operator">(3.5)</a>,
<a class="reference internal" href="#equation-eqn-tensor-networks-swap-resolution">(3.6)</a> can be depicted as</p>
<div class="figure align-center" id="fig-tensor-networks-swap-indices">
<a class="reference internal image-reference" href="../_images/swap-indices.svg"><img alt="../_images/swap-indices.svg" height="100em" src="../_images/swap-indices.svg" /></a>
<p class="caption"><span class="caption-number">Fig. 3.12 </span><span class="caption-text"><span class="math notranslate nohighlight">\(\swap\)</span> operator in action</span><a class="headerlink" href="#fig-tensor-networks-swap-indices" title="Permalink to this image">¶</a></p>
</div>
<p>visualizing the exchanged roles of <span class="math notranslate nohighlight">\(v\)</span> and <span class="math notranslate nohighlight">\(w\)</span>. This leads to the following definition.</p>
<div class="proof definition admonition" id="def-tensor-networks-swap-operator">
<p class="admonition-title"><span class="caption-number">Definition 3.2 </span> (<span class="math notranslate nohighlight">\(\swap\)</span> operator)</p>
<div class="definition-content section" id="proof-content">
<p>The <span class="math notranslate nohighlight">\(\swap\)</span> operator, respectively tensor <span class="math notranslate nohighlight">\(S\)</span> of equation
<a class="reference internal" href="#equation-eqn-tensor-networks-swap-tensor">(3.4)</a>, will be displayed in
tensor network diagram notation as:</p>
<div class="figure align-center" id="fig-tensor-networks-swap-definition">
<a class="reference internal image-reference" href="../_images/swap-definition.svg"><img alt="../_images/swap-definition.svg" height="66em" src="../_images/swap-definition.svg" /></a>
<p class="caption"><span class="caption-number">Fig. 3.13 </span><span class="caption-text"><span class="math notranslate nohighlight">\(\swap\)</span> operator</span><a class="headerlink" href="#fig-tensor-networks-swap-definition" title="Permalink to this image">¶</a></p>
</div>
</div>
</div><div class="proof observation admonition" id="obs-tensor-networks-swap-selfinverse">
<p class="admonition-title"><span class="caption-number">Observation 3.3 </span> (<span class="math notranslate nohighlight">\(\swap\)</span> twice)</p>
<div class="observation-content section" id="proof-content">
<p>The <span class="math notranslate nohighlight">\(\swap\)</span> operator is self-inverse. Swapping the same vectors
twice is nothing different than the identity map.</p>
<div class="figure align-center" id="fig-tensor-networks-swap-selfinverse">
<a class="reference internal image-reference" href="../_images/swap-selfinverse.svg"><img alt="../_images/swap-selfinverse.svg" height="66em" src="../_images/swap-selfinverse.svg" /></a>
<p class="caption"><span class="caption-number">Fig. 3.14 </span><span class="caption-text"><span class="math notranslate nohighlight">\(\swap\)</span> twice</span><a class="headerlink" href="#fig-tensor-networks-swap-selfinverse" title="Permalink to this image">¶</a></p>
</div>
</div>
</div></div>
<div class="section" id="permutations">
<span id="sec-tensor-networks-permutations"></span><h2><span class="section-number">3.3. </span>Permutations<a class="headerlink" href="#permutations" title="Permalink to this headline">¶</a></h2>
<p>Before we continue with tensor networks, we will investigate permutations. This will come handy
when we introduce the Levi-Civita Symbol.</p>
<p>We start with some history and the 15-puzzle. As a reference confer <span id="id4">[<a class="reference internal" href="bibliography.html#id6" title="Jerry Slocum. Sam loyd's most successful hoax. 2009. URL: https://collections.libraries.indiana.edu/lilly/exhibitions_legacy/collections/overview/puzzle_docs/Sam_Loyd_Successful_Hoax.pdf.">Slo09</a>]</span>.
Worcester, Massachusetts, January 1880: Dentist Dr. Pevey published following reward offer.</p>
<div class="figure align-center" id="fig-tensor-networks-permutations-price-offer">
<img alt="../_images/permutations-price-offer.jpg" src="../_images/permutations-price-offer.jpg" />
<p class="caption"><span class="caption-number">Fig. 3.15 </span><span class="caption-text">Award</span><a class="headerlink" href="#fig-tensor-networks-permutations-price-offer" title="Permalink to this image">¶</a></p>
</div>
<p>A set of false teeth and $100 for the successful competitor, who would be able to solve the 15-puzzle.
The puzzle was invented a few years earlier and first sold around Christmas 1879.</p>
<div class="figure align-center" id="fig-tensor-networks-permutations-gem-puzzle">
<img alt="../_images/permutations-gem-puzzle.jpg" src="../_images/permutations-gem-puzzle.jpg" />
<p class="caption"><span class="caption-number">Fig. 3.16 </span><span class="caption-text">Gem Puzzle</span><a class="headerlink" href="#fig-tensor-networks-permutations-gem-puzzle" title="Permalink to this image">¶</a></p>
</div>
<p>The instructions have been very simple: “Place the blocks in the box irregularly,
then move until in regular order.” Maybe you want to try yourself.</p>
<p align="center">
  <iframe src="https://game-15-puzzle.herokuapp.com/" width="600" height="560"></iframe>
</p>
<p>When you hit “scramble”, random moves are executed to create a starting configuration.
Obviously this is always solvable, you just would need to reverse the sequence of moves.
However, back in 1880 using the wooden box and starting with a random placement of blocks,
this turned out to be different. One time people could solve it - and the next time it
seemed impossible. Especially the arrangement, with all the numbers already correctly
placed but numbers 14 and 15 reversed, conveyed the impression to not be solvable.
Still many people claimed it was.</p>
<div class="figure align-center" id="fig-tensor-networks-permutations-unsolvable">
<a class="reference internal image-reference" href="../_images/permutations-unsolvable.png"><img alt="../_images/permutations-unsolvable.png" src="../_images/permutations-unsolvable.png" style="height: 20em;" /></a>
<p class="caption"><span class="caption-number">Fig. 3.17 </span><span class="caption-text">Unsolvable</span><a class="headerlink" href="#fig-tensor-networks-permutations-unsolvable" title="Permalink to this image">¶</a></p>
</div>
<p>Such an argument led Dr. Pevey to offer a price as he described later.</p>
<blockquote>
<div><p>Dr. Pevey Explains. (<em>Worcester Evening Gazette</em>, January 31, 1880)
The reason why I wanted you all to help me work out the puzzle was to convince that girl.
You see she said she worked it out; she knew she did, and if I said she did not,
I simply doubted her veracity. Now to doubt the word of a young lady is high treason and
of course should be punished as such, so I stopped to think how I could convince her
(without putting it into words) that she did not do what she said she did.
From her looks I made up my mind it was no easy task and would probably require the whole
population of Worcester to help me. For she knew she did it! And that you know would
ordinarily settle it, but take what it would or cost what it would, she must be convinced,
but now that we have all given the matter a weeks careful study, and without a single
favorable result. Probably she will no longer contend that she did it.
At first some of us, as you know, rather held to it that it could be done, and that perhaps
she was right. But now that we are all of one mind, that it can not be done, and that we
were mistaken, we will laugh over our week’s fun and proceed to business again.
Respectfully,
Chas. K. Pevey
Pevey’s Dental Rooms, cor. Main and Pleasant Streets, Worcester, Mass.</p>
</div></blockquote>
<p>But is it really bulletproof evidence that a crowd didn’t come up with a solution? Of course not,
otherwise inventions not made yet would never be made. Let’s develop a <em>logical</em> argument that
demystifies the riddle.</p>
<p>We claim that there are two kinds of piece configurations.</p>
<ul class="simple">
<li><p><strong>Solvable</strong>: Those that can be transferred by regular moves into the target configuration.
That is numbers being in order <span class="math notranslate nohighlight">\(1,2,\ldots,15\)</span> (row by row from left to right) with the empty
square located after number <span class="math notranslate nohighlight">\(15\)</span> (in lower right corner).</p></li>
<li><p><strong>Unsolvable</strong>: Those that can not be transferred by regular moves into the target configuration.</p></li>
</ul>
<p>In particular, the configuration derived from target configuration by swapping <span class="math notranslate nohighlight">\(14\)</span> and <span class="math notranslate nohighlight">\(15\)</span> is
unsolvable. To classify piece configurations we’ll construct an invariant that doesn’t change
under moving. This will be done with the help of permutations and inversions.</p>
<div class="proof definition admonition" id="def-tensor-networks-permutation">
<p class="admonition-title"><span class="caption-number">Definition 3.3 </span> (Permutation)</p>
<div class="definition-content section" id="proof-content">
<p>An <span class="math notranslate nohighlight">\(\pmb{n}\)</span>-<em><strong>permutation</strong></em> <span class="math notranslate nohighlight">\(\pi\)</span> is a bijection
<span class="math notranslate nohighlight">\(\pi:\{1,\ldots,n\}\rightarrow\{1,\ldots,n\}\)</span>. If the specific <span class="math notranslate nohighlight">\(n\)</span> is unimportant or implicitely given,
we might use the term <em><strong>permutation</strong></em>.</p>
<p>An <em><strong>inversion</strong></em> of a permutation <span class="math notranslate nohighlight">\(\pi\)</span> is a pair <span class="math notranslate nohighlight">\((i,j)\)</span> with <span class="math notranslate nohighlight">\(i&lt;j\)</span> and <span class="math notranslate nohighlight">\(\pi(i)&gt;\pi(j)\)</span>.</p>
<p>The <em><strong>inversion set</strong></em> ist the set of all inversions.</p>
<p>The <em><strong>inversion number</strong></em> <span class="math notranslate nohighlight">\(\inv(\pi)\)</span> is given by the cardinality of the inversion set</p>
<div class="math notranslate nohighlight">
\[
\inv(\pi)\def\,\#\{(i,j):i&lt;j\text{ and }\pi(i)&gt;\pi(j)\} \,.
\]</div>
</div>
</div><p>To every piece configuration a permutation of pieces can be assigned. We form a sequence
of pieces by reading the first row from left to right, then the second row from left to
right and so on (the empty square is to be ignored). The resulting sequence is considered
to be <span class="math notranslate nohighlight">\(\pi(1),\pi(2),\ldots,\pi(15)\)</span>.</p>
<p>For example, to the target configuration the identity map <span class="math notranslate nohighlight">\(\id:\id(k)=k,\,\forall k\)</span> is
assigned, <span class="math notranslate nohighlight">\(\inv(\id)=0\)</span>. To the configuration with <span class="math notranslate nohighlight">\(14\)</span> and <span class="math notranslate nohighlight">\(15\)</span> being swapped, the permutation</p>
<div class="math notranslate nohighlight">
\[\tau:\tau(k)=k,\,\forall k\le 13,\;\tau(14)=15,\,\tau(15)=14\]</div>
<p>is assigned, <span class="math notranslate nohighlight">\(\inv(\tau)=1\)</span>.</p>
<div class="proof observation admonition" id="obs-tensor-networks-invariant">
<p class="admonition-title"><span class="caption-number">Observation 3.4 </span> (Invariant)</p>
<div class="observation-content section" id="proof-content">
<p>For a given piece configuration let <span class="math notranslate nohighlight">\(\pi\)</span> denote the assigned permutation and let
<span class="math notranslate nohighlight">\(r:1\le r\le 4\)</span> denote the row number of the empty square. Then</p>
<div class="math notranslate nohighlight">
\[\inv(\pi)+r\mod 2\]</div>
<p>is invariant under any legal move.</p>
</div>
</div><div class="proof admonition" id="proof">
<p>Proof. First of all, sliding a piece in the same row changes neither the number of inversions nor
the row number of the empty square. Therefore let’s consider moving a piece down (moving up
is following the same argument).</p>
<p>What happens to the assigned permutation? Consider the sequence of pieces we used during
construction. The moving piece is shifted three positions towards the end of sequence.
Hence, exactly three swaps of piece pairs occur. Each swap will impose a change of inversion
status of the affected pair. Either the pair will be turned from being in order into an
inversion or vice versa. Since there are three swaps, the inversion number will change parity.</p>
<p>On the other hand the empty square moves up, causing the row number to change parity as well.
Therefore the parity of <span class="math notranslate nohighlight">\(\inv(\pi)+r\)</span> doesn’t change.</p>
</div>
<p>The rest is simple. To be solvable a piece configuration needs to have same parity of
<span class="math notranslate nohighlight">\(\inv(\pi)+r\)</span> as the target configuration, that is even. Swapping <span class="math notranslate nohighlight">\(14\)</span> and <span class="math notranslate nohighlight">\(15\)</span> yields odd
<span class="math notranslate nohighlight">\(\inv(\tau)+r\)</span>. Therefore no sequence of legal moves exists to transfer this configuration
into the target configuration.</p>
<p>Let’s continue with another quadratic playing field, this time a matrix <span class="math notranslate nohighlight">\(A\in\C^{n\times n}\)</span>.
This time the classes are:</p>
<ul class="simple">
<li><p><strong>Solvable</strong>: <span class="math notranslate nohighlight">\(A\)</span> is invertible, i.e. there is a matrix <span class="math notranslate nohighlight">\(A^{-1}\in\C^{n\times n}\)</span> with
<span class="math notranslate nohighlight">\(AA^{-1}=A^{-1}A=I\)</span>.</p></li>
<li><p><strong>Unsolvable</strong>: Aforementioned matrix <span class="math notranslate nohighlight">\(A^{-1}\)</span> is not existing.</p></li>
</ul>
<p>Again we will distinguish both classes be constructing an appropriate invariant.</p>
<div class="proof definition admonition" id="def-tensor-networks-permutation-parity">
<p class="admonition-title"><span class="caption-number">Definition 3.4 </span> (Parity of permutation)</p>
<div class="definition-content section" id="proof-content">
<p>The <em><strong>parity</strong></em> <span class="math notranslate nohighlight">\(\sgn(\pi)\)</span> of a permutation <span class="math notranslate nohighlight">\(\pi\)</span> is defined by</p>
<div class="math notranslate nohighlight">
\[
\sgn(\pi)\def(-1)^{\inv(\pi)}\,.
\]</div>
</div>
</div><div class="proof observation admonition" id="obs-tensor-networks-parity-swap">
<p class="admonition-title"><span class="caption-number">Observation 3.5 </span> (Parity swap)</p>
<div class="observation-content section" id="proof-content">
<p>If we swap two values of a permutation, then the parity changes. That is, if
<span class="math notranslate nohighlight">\(\pi\)</span> and <span class="math notranslate nohighlight">\(\pi'\)</span> are <span class="math notranslate nohighlight">\(n\)</span>-permutations with</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\pi(i) &amp;= \pi'(j) \,, \\
\pi(j) &amp;= \pi'(i) \,, \\
\pi(k) &amp;= \pi'(k) \,,\, i\neq k\neq j
\end{split}\]</div>
<p>for some <span class="math notranslate nohighlight">\(i\)</span> and <span class="math notranslate nohighlight">\(j\)</span>, then</p>
<div class="math notranslate nohighlight">
\[
\sgn(\pi) = -\sgn(\pi') \,.
\]</div>
</div>
</div><div class="proof definition admonition" id="def-tensor-networks-determinant">
<p class="admonition-title"><span class="caption-number">Definition 3.5 </span> (Determinant)</p>
<div class="definition-content section" id="proof-content">
<p>We denote the set of all <span class="math notranslate nohighlight">\(n\)</span>-permutations by <span class="math notranslate nohighlight">\(S_n\)</span>. We define the <em><strong>determinant</strong></em>
of a matrix <span class="math notranslate nohighlight">\(A=\lbrack a_{ij}\rbrack_{i=1,j=1}^{n,\;\;\;n}\in\C^{n\times n}\)</span> by</p>
<div class="math notranslate nohighlight">
\[
\det(A)\def\sum_{\pi\in S_n}\sgn(\pi)\cdot a_{\pi(1)1}\cdots a_{\pi(n)n}\,.
\]</div>
</div>
</div><p>This formula will become much clearer, when writing out for small or sparse matrices.</p>
<div class="proof example admonition" id="ex-tensor-networks-det1x1">
<p class="admonition-title"><span class="caption-number">Example 3.1 </span> (Determinant of <span class="math notranslate nohighlight">\(1\times 1\)</span>-matrix)</p>
<div class="example-content section" id="proof-content">
<p>Let <span class="math notranslate nohighlight">\(A=[a]\)</span>. The only permutation of one element is <span class="math notranslate nohighlight">\(\id\)</span> with <span class="math notranslate nohighlight">\(\sgn(\id)=1\)</span>.
We get</p>
<div class="math notranslate nohighlight">
\[
\det(A)=1\cdot a=a\,.
\]</div>
</div>
</div><div class="proof example admonition" id="ex-tensor-networks-det2x2">
<p class="admonition-title"><span class="caption-number">Example 3.2 </span> (Determinant of <span class="math notranslate nohighlight">\(2\times 2\)</span>-matrix)</p>
<div class="example-content section" id="proof-content">
<p>Let <span class="math notranslate nohighlight">\(A=\begin{bmatrix} a &amp; b \\ c &amp; d \end{bmatrix}\)</span>. We have two permutations,
<span class="math notranslate nohighlight">\(\id\)</span> and <span class="math notranslate nohighlight">\(\pi:\pi(1)=2,\pi(2)=1\)</span> with <span class="math notranslate nohighlight">\(\sgn(\pi)=-1\)</span>. We get</p>
<div class="math notranslate nohighlight">
\[
\det(A)=1\cdot a\cdot d+(-1)\cdot c\cdot b=ad-bc\,.
\]</div>
</div>
</div><div class="proof example admonition" id="ex-tensor-networks-det3x3">
<p class="admonition-title"><span class="caption-number">Example 3.3 </span> (Determinant of <span class="math notranslate nohighlight">\(3\times 3\)</span>-matrix)</p>
<div class="example-content section" id="proof-content">
<ul class="simple">
<li><p>Let
<span class="math notranslate nohighlight">\(A=\begin{bmatrix} a_{11} &amp; a_{12} &amp; a_{13} \\ a_{21} &amp; a_{22} &amp; a_{23} \\ a_{31} &amp; a_{32} &amp; a_{33} \end{bmatrix}\)</span>.
We get</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
\det(A)=
a_{11}a_{22}a_{33}-a_{11}a_{32}a_{23}-a_{21}a_{12}a_{33}+
a_{21}a_{32}a_{13}+a_{31}a_{12}a_{23}-a_{31}a_{22}a_{13}\,.
\]</div>
</div>
</div><div class="proof example admonition" id="ex-tensor-networks-detdiag">
<p class="admonition-title"><span class="caption-number">Example 3.4 </span> (Determinant of diagonal matrix)</p>
<div class="example-content section" id="proof-content">
<p>Let <span class="math notranslate nohighlight">\(A=\diag(a_1,\ldots,a_n)\)</span>. All permutations but <span class="math notranslate nohighlight">\(\id\)</span> contain a zero-element in the
respective product. Hence only one addend remains and we get</p>
<div class="math notranslate nohighlight">
\[
\det(A)=a_1\cdot\ldots\cdot a_n\,.
\]</div>
</div>
</div><div class="proof example admonition" id="ex-tensor-networks-dettriangular">
<p class="admonition-title"><span class="caption-number">Example 3.5 </span> (Determinant of triangular matrix)</p>
<div class="example-content section" id="proof-content">
<p>The same reasoning as in <a class="reference internal" href="#ex-tensor-networks-detdiag">Example 3.4</a> can be applied. Again only
the main diagonal addend survives and the determinant is given by the product of the main
diagonal entries.</p>
</div>
</div><p>It’s time to derive some properties from the definition.</p>
<div class="proof observation admonition" id="obs-tensor-networks-determinant-multilinearity">
<p class="admonition-title"><span class="caption-number">Observation 3.6 </span> (Determinant properties)</p>
<div class="observation-content section" id="proof-content">
<p>We regard matrices <span class="math notranslate nohighlight">\(A=\lbrack a_{ij}\rbrack_{i=1,j=1}^{n,\;\;\;n}\in\C^{n\times n}\)</span> as
being composed of its <span class="math notranslate nohighlight">\(n\)</span> columns, so denoted as <span class="math notranslate nohighlight">\(A=[A_1\ldots A_n]\)</span>. The column vector
<span class="math notranslate nohighlight">\(A_j\)</span> is composed of the entries of the <span class="math notranslate nohighlight">\(j\)</span>-th column <span class="math notranslate nohighlight">\((A_j)_i=a_{ij}\)</span>. Thus,
the determinant <span class="math notranslate nohighlight">\(\det(A)\)</span> can be considered as function of the <span class="math notranslate nohighlight">\(n\)</span> columns
<span class="math notranslate nohighlight">\(\det(A)=f(A_1,\ldots,A_n)\)</span>. This function has the following properties:</p>
<p><span class="math notranslate nohighlight">\(f\)</span> is <em><strong>multilinear</strong></em>, i.e. <span class="math notranslate nohighlight">\(f\)</span> is linear for every column <span class="math notranslate nohighlight">\(A_i\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
f(A_1,\ldots,\lambda A_i,\ldots,A_n)&amp;=\lambda f(A_1,\ldots,A_i,\ldots,A_n)\,, \\
f(A_1,\ldots,A_i+B_i,\ldots,A_n)&amp;=f(A_1,\ldots,A_i,\ldots,A_n)+f(A_1,\ldots,B_i,\ldots,A_n)\,.
\end{split}\]</div>
<p><span class="math notranslate nohighlight">\(f\)</span> is <em><strong>alternating</strong></em>, i.e. swapping two columns will change the sign:</p>
<div class="math notranslate nohighlight">
\[
f(A_1,\ldots,A_i,\ldots,A_j,\ldots,A_n)=-f(A_1,\ldots,A_j,\ldots,A_i,\ldots,A_n)\,.
\]</div>
</div>
</div><div class="proof admonition" id="proof">
<p>Proof. Multilinearity can be shown straight forward from definition of determinant.</p>
<div class="math notranslate nohighlight">
\[\begin{split}
f(A_1,\ldots,\lambda A_i,\ldots,A_n)
&amp;= \sum_{\pi\in S_n}\sgn(\pi)\cdot a_{\pi(1)1}\cdots\lambda a_{\pi(i)i}\cdots a_{\pi(n)n} \\
&amp;= \lambda\sum_{\pi\in S_n}\sgn(\pi)\cdot a_{\pi(1)1}\cdots a_{\pi(i)i}\cdots a_{\pi(n)n} \\
&amp;= \lambda f(A_1,\ldots,A_i,\ldots,A_n)\,,
\end{split}\]</div>
<div class="math notranslate nohighlight">
\[\begin{split}
f(A_1,\ldots,A_i+B_i,\ldots,A_n)
&amp;= \sum_{\pi\in S_n}\sgn(\pi)\cdot a_{\pi(1)1}\cdots(a_{\pi(i)i}+b_{\pi(i)i})\cdots a_{\pi(n)n} \\
&amp;= \sum_{\pi\in S_n}\sgn(\pi)\cdot a_{\pi(1)1}\cdots a_{\pi(i)i}\cdots a_{\pi(n)n} \\
&amp;\quad +\sum_{\pi\in S_n}\sgn(\pi)\cdot a_{\pi(1)1}\cdots b_{\pi(i)i}\cdots a_{\pi(n)n} \\
&amp;= f(A_1,\ldots,A_i,\ldots,A_n)+f(A_1,\ldots,B_i,\ldots,A_n)\,.
\end{split}\]</div>
<p>For proving the alternating property, we first assume that the two columns <span class="math notranslate nohighlight">\(A_i\)</span> and <span class="math notranslate nohighlight">\(A_j\)</span>
are adjacent. Swapping them correlates to replacing in each addend permutation
<span class="math notranslate nohighlight">\(\pi\)</span> by</p>
<div class="math notranslate nohighlight">
\[
\pi':\pi'(i)=\pi(j),\,\pi'(j)=\pi(i),\,\pi'(k)=\pi(k),\forall k\neq i,j\,.
\]</div>
<p>By construction <span class="math notranslate nohighlight">\(\pi'\)</span> has the same inversions as <span class="math notranslate nohighlight">\(\pi\)</span> but the status of <span class="math notranslate nohighlight">\((i,j)\)</span> is flipped.
Thus, <span class="math notranslate nohighlight">\(\sgn(\pi')=-\sgn(\pi)\)</span>. This implies the assertion for adjacent columns.</p>
<p>If <span class="math notranslate nohighlight">\(A_i\)</span> and <span class="math notranslate nohighlight">\(A_j\)</span> are not adjacent, the swap can be executed by consecutive execution of
adjacent swaps. Similar to bubble sort, move <span class="math notranslate nohighlight">\(A_i\)</span> next to <span class="math notranslate nohighlight">\(A_j\)</span>. Swap both and move <span class="math notranslate nohighlight">\(A_j\)</span>
back to the initial position of <span class="math notranslate nohighlight">\(A_i\)</span>. The procedure takes an odd number of swaps (twice
the movement plus the swap of <span class="math notranslate nohighlight">\(A_i\)</span> and <span class="math notranslate nohighlight">\(A_j\)</span>). This yields the full assertion.</p>
</div>
<p>From this result we can draw two simple yet powerful consequences.</p>
<div class="proof corollary admonition" id="cor-tensor-networks-determinant-id-cols">
<p class="admonition-title"><span class="caption-number">Corollary 3.1 </span> (Identical columns)</p>
<div class="corollary-content section" id="proof-content">
<p>If two columns of <span class="math notranslate nohighlight">\(A\)</span> are the same, then <span class="math notranslate nohighlight">\(\det(A)=0\)</span>.</p>
</div>
</div><div class="proof admonition" id="proof">
<p>Proof. We use the alternating property and swap the equal columns. This would not change <span class="math notranslate nohighlight">\(A\)</span> but
the sign of its determinant. Hence, zero is the only possible value.</p>
</div>
<div class="proof corollary admonition" id="cor-tensor-networks-determinant-el-ops">
<p class="admonition-title"><span class="caption-number">Corollary 3.2 </span> (Elementary operations)</p>
<div class="corollary-content section" id="proof-content">
<p>If to one column of <span class="math notranslate nohighlight">\(A\)</span> a linear combination of other columns is added,
then the determinant doesn’t change.</p>
</div>
</div><div class="proof admonition" id="proof">
<p>Proof. We use multilinearity and the previous corollary. Taking apart the linear combination into
a sum of determinants leaves only the original determinant. The other addends are zero due
to duplication of a column.</p>
</div>
<p>Now we are ready to formulate the invariant.</p>
<div class="proof theorem admonition" id="thm-tensor-networks-invertibility">
<p class="admonition-title"><span class="caption-number">Theorem 3.1 </span> (Invertibility criteria)</p>
<div class="theorem-content section" id="proof-content">
<p>A matrix <span class="math notranslate nohighlight">\(A\in\C^{n\times n}\)</span> is invertible if and only if <span class="math notranslate nohighlight">\(\det(A)\neq 0\)</span>.</p>
</div>
</div><div class="proof admonition" id="proof">
<p>Proof. We use Gaussian elimination to try to obtain the inverse to <span class="math notranslate nohighlight">\(A\)</span>. Usually Gaussian
elemination is executed row-wise, but due to the fact that <span class="math notranslate nohighlight">\((A^{-1})^\dagger=(A^\dagger)^{-1}\)</span>
we can apply it column-wise. The previous corollary ensures, that the elimination steps will
not modify the determinant. We will run into either one of the following cases:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(A\)</span> is invertible. In this case Gaussian elimination will yield a triangular matrix with main
diagonal entries being non-zero. Consequently, <span class="math notranslate nohighlight">\(\det(A)\neq 0\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(A\)</span> is not invertible. In this case Gaussian elimination will yield a zero-column. Hence,
every product of the determinant definition has a zero factor. We obtain <span class="math notranslate nohighlight">\(\det(A)=0\)</span>.</p></li>
</ul>
<p>This proves the theorem.</p>
</div>
</div>
<div class="section" id="graphical-reasoning">
<h2><span class="section-number">3.4. </span>Graphical Reasoning<a class="headerlink" href="#graphical-reasoning" title="Permalink to this headline">¶</a></h2>
<p>Another helpful tool using the parity of permutations is the <span class="math notranslate nohighlight">\(\varepsilon\)</span><em>-tensor</em>.</p>
<div class="proof definition admonition" id="def-tensor-networks-epstensor">
<p class="admonition-title"><span class="caption-number">Definition 3.6 </span> (<span class="math notranslate nohighlight">\(\varepsilon\)</span>-tensor)</p>
<div class="definition-content section" id="proof-content">
<p>For indices <span class="math notranslate nohighlight">\(i_1,\ldots,i_r\)</span> we define the <em><strong>Levi-Civita symbol</strong></em> as</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\varepsilon^{i_1\ldots i_r}\def
\begin{cases}
  \sgn(\pi)\,, 
    &amp; \text{index values are an $r$-permutation $\pi$ with $i_k=\pi(k),\,k=1,\ldots,r$} 
    \,, \\
  0\,, &amp; \text{otherwise} \,.
\end{cases}
\end{split}\]</div>
<p>The <span class="math notranslate nohighlight">\(\bold{\varepsilon}\)</span><em><strong>-tensor</strong></em> is a tensor based on the Levi-Civita symbol.</p>
<div class="math notranslate nohighlight">
\[
\varepsilon\def\sum\limits_{i_1=1}^{n_1}\cdots\sum\limits_{i_r=1}^{n_r}
  \varepsilon^{i_1\ldots i_r}\,e_{i_1}\otimes\ldots\otimes e_{i_r} \,.
\]</div>
</div>
</div><p>We have defined the <span class="math notranslate nohighlight">\(\varepsilon\)</span>-tensor using contravariant indices
(superscript at coefficients), but similar to the Kronecker symbol we
will be using the <span class="math notranslate nohighlight">\(\varepsilon\)</span>-tensor with all kinds of index combinations.
For example,</p>
<div class="math notranslate nohighlight" id="equation-eq-tensor-networks-epsraw">
<span class="eqno">(3.8)<a class="headerlink" href="#equation-eq-tensor-networks-epsraw" title="Permalink to this equation">¶</a></span>\[\begin{split}\varepsilon
&amp;= \sum\limits_{i=1}^2\sum\limits_{j=1}^2\varepsilon_{ij}\,e^i\otimes e^j \\
&amp;= e^1\otimes e^2-e^2\otimes e^1\,\end{split}\]</div>
<p>is also an <span class="math notranslate nohighlight">\(\varepsilon\)</span>-tensor making use of the Levi-Civita symbol with
subscript indices. The <span class="math notranslate nohighlight">\(\varepsilon\)</span>-tensor will usually used in conjunction
with other tensors that define its type. Therefore we will be using a round
shape, for instance the tensor of equation <a class="reference internal" href="#equation-eq-tensor-networks-epsraw">(3.8)</a> will
look like (using wire bending):</p>
<div class="figure align-center" id="fig-tensor-networks-epsilon-raw">
<a class="reference internal image-reference" href="../_images/epsilon-raw.svg"><img alt="../_images/epsilon-raw.svg" height="110em" src="../_images/epsilon-raw.svg" /></a>
<p class="caption"><span class="caption-number">Fig. 3.18 </span><span class="caption-text"><span class="math notranslate nohighlight">\(\varepsilon\)</span>-tensor</span><a class="headerlink" href="#fig-tensor-networks-epsilon-raw" title="Permalink to this image">¶</a></p>
</div>
<p>From <a class="reference internal" href="#obs-tensor-networks-parity-swap">Observation 3.5</a> we learn that the <span class="math notranslate nohighlight">\(\varepsilon\)</span>-tensor
will change its sign, if the position of two coefficient’s indices is swapped.
For example, again using the tensor of <a class="reference internal" href="#equation-eq-tensor-networks-epsraw">(3.8)</a>, we have</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\varepsilon'
&amp;\def \sum\limits_{i=1}^2\sum\limits_{j=1}^2\varepsilon_{ji}\,e^i\otimes e^j \\
&amp;= \sum\limits_{i=1}^2\sum\limits_{j=1}^2(-\varepsilon_{ij})\,e^i\otimes e^j \\
&amp;= -\varepsilon\,.
\end{split}\]</div>
<p>This can of course also be expressed as diagram:</p>
<div class="figure align-center" id="fig-tensor-networks-epsilon-swap">
<a class="reference internal image-reference" href="../_images/epsilon-swap.svg"><img alt="../_images/epsilon-swap.svg" height="80em" src="../_images/epsilon-swap.svg" /></a>
<p class="caption"><span class="caption-number">Fig. 3.19 </span><span class="caption-text">Index swap</span><a class="headerlink" href="#fig-tensor-networks-epsilon-swap" title="Permalink to this image">¶</a></p>
</div>
<div class="proof definition admonition" id="definition-27">
<p class="admonition-title"><span class="caption-number">Definition 3.7 </span> (Antisymmetric tensor)</p>
<div class="definition-content section" id="proof-content">
<p>In accordance with
<a class="reference external" href="https://en.wikipedia.org/wiki/Skew-symmetric_matrix"><em>antisymmetric matrices</em></a>,
we call a tensor that switches sign under index swap <em><strong>antisymmetric tensor</strong></em>.</p>
</div>
</div><p>Let</p>
<div class="math notranslate nohighlight" id="equation-eq-tensor-networks-2x2t">
<span class="eqno">(3.9)<a class="headerlink" href="#equation-eq-tensor-networks-2x2t" title="Permalink to this equation">¶</a></span>\[T=\sum\limits_{i=1}^2\sum\limits_{j=1}^2T^i_j\,e_i\otimes e^j\]</div>
<p>be a tensor. We know from <a class="reference internal" href="#ex-tensor-networks-det2x2">Example 3.2</a> how we can
compute the determinant of its coefficients matrix, which we will denote
as <span class="math notranslate nohighlight">\(\det(T)\)</span> here.</p>
<div class="math notranslate nohighlight" id="equation-eq-tensor-networks-epsdet">
<span class="eqno">(3.10)<a class="headerlink" href="#equation-eq-tensor-networks-epsdet" title="Permalink to this equation">¶</a></span>\[\begin{split}\det(T)
&amp;= T^1_1T^2_2-T^2_1T^1_2 \\
&amp;= \sum\limits_{i=1}^2\sum\limits_{j=1}^2\varepsilon^{ij}T^1_iT^2_j \,.\end{split}\]</div>
<p>Now, check this out.</p>
<div class="proof observation admonition" id="observation-28">
<p class="admonition-title"><span class="caption-number">Observation 3.7 </span> (Determinant diagram)</p>
<div class="observation-content section" id="proof-content">
<p>With <span class="math notranslate nohighlight">\(T\)</span> of equation <a class="reference internal" href="#equation-eq-tensor-networks-2x2t">(3.9)</a>, the following diagrammatic equation
holds.</p>
<div class="figure align-center" id="fig-tensor-networks-epsilon-det">
<a class="reference internal image-reference" href="../_images/epsilon-det.jpg"><img alt="../_images/epsilon-det.jpg" src="../_images/epsilon-det.jpg" style="height: 20em;" /></a>
<p class="caption"><span class="caption-number">Fig. 3.20 </span><span class="caption-text">Determinant equation</span><a class="headerlink" href="#fig-tensor-networks-epsilon-det" title="Permalink to this image">¶</a></p>
</div>
</div>
</div><div class="proof admonition" id="proof">
<p>Proof. We transform the left-hand side into the ride-hand side.</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\sum\limits_{i=1}^2\sum\limits_{k=1}^2\sum\limits_{j=1}^2\sum\limits_{l=1}^2
  \varepsilon^{jl}T^i_jT^k_l\,e_i\otimes e_k
&amp;= \underbrace{
    \sum\limits_{j=1}^2\sum\limits_{l=1}^2\varepsilon^{jl}T^1_jT^1_l\,e_1\otimes e_1
  }_{=(T^1_1T^1_2-T^1_2T^1_1)\,e_1\otimes e_1=0} \\
&amp;\quad + \underbrace{
    \sum\limits_{j=1}^2\sum\limits_{l=1}^2\varepsilon^{jl}T^1_jT^2_l\,e_1\otimes e_2
  }_{=\det(T)\,e_1\otimes e_2} \\
&amp;\quad + \underbrace{
    \sum\limits_{j=1}^2\sum\limits_{l=1}^2\varepsilon^{jl}T^2_jT^1_l\,e_2\otimes e_1
  }_{=-\det(T)\,e_2\otimes e_1} \\
&amp;\quad + \underbrace{
    \sum\limits_{j=1}^2\sum\limits_{l=1}^2\varepsilon^{jl}T^2_jT^2_l\,e_2\otimes e_2
  }_{=(T^2_1T^2_2-T^2_2T^2_1)\,e_2\otimes e_2=0} \\
&amp;=\det(T)\sum\limits_{i=1}^2\sum\limits_{k=1}^2\varepsilon^{ik}\,e_i\otimes e_k \\
&amp;= \det(T)\cdot\varepsilon\,.
\end{split}\]</div>
</div>
<p>Let <span class="math notranslate nohighlight">\(\psi\in\C^2\otimes\C^2\)</span> be a vector of a tensor product space. <span class="math notranslate nohighlight">\(\psi\)</span> can be
entangled or separable. Similar to the determinant being an invariant for invertible
matrices, we will now define an invariant for entangled/separable tensors.</p>
<div class="proof definition admonition" id="def-tensor-networks-concurrence">
<p class="admonition-title"><span class="caption-number">Definition 3.8 </span> (Concurrence)</p>
<div class="definition-content section" id="proof-content">
<p>We define the <em><strong>concurrence</strong></em> <span class="math notranslate nohighlight">\(C(\psi)\)</span> of <span class="math notranslate nohighlight">\(\psi\in\C^2\otimes\C^2\)</span> as the absolute
value of the following full contracted tensor network (therefore it is a real number).</p>
<div class="figure align-center" id="fig-tensor-networks-epsilon-concurrence">
<a class="reference internal image-reference" href="../_images/epsilon-concurrence.jpg"><img alt="../_images/epsilon-concurrence.jpg" src="../_images/epsilon-concurrence.jpg" style="height: 20em;" /></a>
<p class="caption"><span class="caption-number">Fig. 3.21 </span><span class="caption-text">Concurrence</span><a class="headerlink" href="#fig-tensor-networks-epsilon-concurrence" title="Permalink to this image">¶</a></p>
</div>
</div>
</div><p>The concurrence measures, if a tensor is entangled.</p>
<div class="proof observation admonition" id="obs-tensor-networks-concurrence">
<p class="admonition-title"><span class="caption-number">Observation 3.8 </span> (Concurrence as invariant)</p>
<div class="observation-content section" id="proof-content">
<p><span class="math notranslate nohighlight">\(\psi\in\C^2\otimes\C^2\)</span> is entangled if and only if <span class="math notranslate nohighlight">\(C(\psi)\neq 0\)</span>.</p>
</div>
</div><div class="proof admonition" id="proof">
<p>Proof. We have</p>
<div class="figure align-center" id="fig-tensor-networks-epsilon-concurrence-eq">
<a class="reference internal image-reference" href="../_images/epsilon-concurrence-eq.jpg"><img alt="../_images/epsilon-concurrence-eq.jpg" src="../_images/epsilon-concurrence-eq.jpg" style="height: 50em;" /></a>
<p class="caption"><span class="caption-number">Fig. 3.22 </span><span class="caption-text">Concurrence equation</span><a class="headerlink" href="#fig-tensor-networks-epsilon-concurrence-eq" title="Permalink to this image">¶</a></p>
</div>
</div>
<p>Final step:</p>
<div class="figure align-center" id="fig-tensor-networks-epsilon-local-trafo">
<a class="reference internal image-reference" href="../_images/epsilon-local-trafo.jpg"><img alt="../_images/epsilon-local-trafo.jpg" src="../_images/epsilon-local-trafo.jpg" style="height: 50em;" /></a>
<p class="caption"><span class="caption-number">Fig. 3.23 </span><span class="caption-text">Local transformation</span><a class="headerlink" href="#fig-tensor-networks-epsilon-local-trafo" title="Permalink to this image">¶</a></p>
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./doc"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            
                <!-- Previous / next buttons -->
<div class='prev-next-area'> 
    <a class='left-prev' id="prev-link" href="tensors.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title"><span class="section-number">2. </span>Tensors</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="combinatorics.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">4. </span>Combinatorics</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            
        </div>
    </div>
    <footer class="footer">
  <p>
    
      By Markus Wappler, Enterprise Application Development Group, University of Applied Sciences Zittau/Görlitz<br/>
    
        &copy; Copyright 2021.<br/>
  </p>
</footer>
</main>


      </div>
    </div>
  
  <script src="../_static/js/index.be7d3bbb2ef33a8344ce.js"></script>

  </body>
</html>